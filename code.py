# -*- coding: utf-8 -*-
"""modified.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zWR2Rnqmnx-V3YY_L6_obu_-i9ekJ_gI
"""

!pip install xgboost seaborn --quiet

import numpy as np
import pandas as pd
import scipy
import io
from google.colab import files
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, RepeatedKFold
import seaborn as sns

pd.options.mode.chained_assignment = None

train_url = 'https://raw.githubusercontent.com/RishabhRJain/Dataset/master/train_file.csv'
test_url = 'https://raw.githubusercontent.com/RishabhRJain/Dataset/master/test_file.csv'

df = pd.read_csv(train_url, sep = ',')
test_df = pd.read_csv(test_url, sep = ',')

#  from google.colab import files
#  uploaded = files.upload()

"""#### Read train data"""

# df = pd.read_csv('./train_file.csv', sep = ',')
# df = pd.read_csv(io.BytesIO(uploaded['train_file.csv']))

df.head()



print("Info about Data:")
print("Number of rows: ", df.shape[0])
print("Number of Columns: ", df.shape[1])
print("\nNumber of null values in each column:\n", df.isnull().sum())

y_train = df['Category']
df = df.drop('Category' , axis = 1)

"""#### Read Test Data"""

# test_df = pd.read_csv('./test_file.csv', sep = ',')

print("Info about Data:")
print("Number of rows: ", test_df.shape[0])
print("Number of Columns: ", test_df.shape[1])

"""#### Merge train and test data set"""

train_test = pd.concat([df, test_df], join = 'inner')
train_test.head()

print("Info about Data:")
print("Number of rows: ", train_test.shape[0])
print("Number of Columns: ", train_test.shape[1])
print("\n Number of null values in both train and test data: \n", train_test.isnull().sum())

"""## Data clean

#### Drop columns which we are sparse, redundant or if you think they dont help in predicting Target category
"""

del_columns = ['Master Use Permit', 'Applicant Name', 'Permit and Complaint Status URL', 
                'Location', 'Issue Date', 'Final Date']
train_test.drop(del_columns, axis = 1, inplace = True)

"""#### check number of unique values and categories in categorical columns"""

train_test.nunique()

binary_cols =  train_test.nunique()[train_test.nunique().values == 2].keys().tolist()

# Maximum categories are for Action type. Hence threshold for categorical columns has been kept at 20
categorical_cols = train_test.nunique()[train_test.nunique().values <= 20].keys().tolist()
categorical_cols = [col for col in categorical_cols if col not in binary_cols]

date_cols = ['Application Date',  'Expiration Date']

location_cols = ['Latitude', 'Longitude' ]

text_cols = ['Address', 'Description', 'Contractor' ]

print("Binary columns are: ", binary_cols)
print("Categorical columns are: ", categorical_cols)
print("Date columns are: ", date_cols)
print("Location columns are: ", location_cols)
print("Text columns are: ", text_cols)

"""#### convert date_cols to Datetime"""

for col in date_cols:
  train_test[col] = pd.to_datetime(train_test[col])

"""#### Remove or replace NaN values"""

for col in categorical_cols + text_cols:
    train_test[col].fillna('NA', inplace = True)
   
for col in location_cols:
    train_test[col].fillna(train_test[col].mean(), inplace = True)

train_test.isnull().sum()

for col in categorical_cols:
    print("\n{}: {} \n {}".format(col, train_test[col].nunique(), train_test[col].unique()))

"""#### Before one hot encoding, create a train copy to be used for visualisation"""

df_copy = train_test[:len(y_train)]
df_copy['Category'] = y_train

"""#### one hot encode categorical and binary columns in train and test data"""

train_test = pd.get_dummies(train_test, columns = categorical_cols)

le_bin = LabelEncoder()
for col in binary_cols:
    train_test[col] = le_bin.fit_transform(train_test[col])

"""## Visualisation"""

df_copy.head()

catplot = sns.catplot(x="Category", kind="count", data=df_copy)
catplot.set_xticklabels(rotation=45)
catplot.set_titles('Category' )
catplot.savefig('./category.png')

"""##### As we see above, we have an imbalanced Dataset"""

work_plot = sns.catplot(x="Work Type", kind="count", data=df_copy)
work_plot.set_xticklabels(rotation=45)
work_plot.set_titles('Work Type' )

status_plot = sns.catplot(x="Status", kind="count", data=df_copy)
status_plot.set_xticklabels(rotation=90)
status_plot.set_titles('Status' )

"""## Copy clean train data back from train_test dataframe"""

df = train_test[:len(y_train)]
df['Category'] = y_train

"""#### create a date dataframe to understand the time difference between expiration and application date based on Category"""

#drop rows in which Application or Expiration Date is empty
date_df = df.dropna(subset=date_cols, how='any')

date_df['total period'] = (date_df['Expiration Date'] - date_df['Application Date']).dt.days

"""#### We see that different categories have time periods for Expiration Date with Multi family Categories having longest duration"""

diff = date_df.groupby(['Category'])['total period'].mean()
diff

"""#### We will use the above data for imputing NaN date values in train data based on categories"""

# Mean time  for Total period across all Categories. We use this to append NaN values in test data
diff.mean()

"""#### Finding time difference in days for train data

#### Fill NaN values in total period column with mean value based on Category
"""

df['total period'] = (df['Expiration Date'] - df['Application Date']).dt.days

df['total period'] = df.apply(
    lambda row: diff[row['Category']] if np.isnan(row['total period']) else df['total period'], axis=1)

df.drop(date_cols, axis = 1, inplace = True)

"""#### use label encoding for Target variable"""

le = LabelEncoder()
target = le.fit_transform(df['Category'])
df.drop('Category', axis = 1, inplace = True)

le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)

df['Latitude'].fillna((df['Latitude'].mean()), inplace=True)
df['Longitude'].fillna((df['Longitude'].mean()), inplace=True)

df.head()

"""### Encoding Address column into a vector"""

#To convert some float values in Address column to string
df['Address'] = df['Address'].astype(str)

def modify_address(row):
  '''
  Takes complete address and returns only street name without spaces. 
  Modified address can be encoded and used to predict Category
  '''
  addr = row.split(" ", 1)
  if len(addr) > 1:
    addr = addr[1].replace(' ', '')
  else:
    addr = addr[0].replace(' ', '')
  return addr

##For example try this:
#(df['Address'][0].split(" ", 1)[1]).replace(' ', '')

df['Address'] = df['Address'].apply(modify_address)

addr_vectorizer = CountVectorizer(min_df = 2) 
addr_vector = addr_vectorizer.fit_transform(df['Address'].tolist())  
addr_vector

"""### Encoding Description column into a vector

#### Using Count Vectorizer for Description column
"""

desc_vectorizer = CountVectorizer(min_df=5 , stop_words='english') 
desc_vector = desc_vectorizer.fit_transform(df['Description'].tolist())  
desc_vector

"""### Encoding Contractor into a vector

#### Using Count Vectorizer for Contractor column
"""

contract_vectorizer = CountVectorizer(min_df = 2) 
contract_vector = contract_vectorizer.fit_transform(df['Contractor'].tolist())  
contract_vector

vectors = scipy.sparse.hstack((addr_vector, desc_vector, contract_vector))
vectors

"""### Approach 1: Building model on text vectors and using its output as input to next model containing numerical and categorical features

#### Finding best model using stratified cross validation
"""

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression

params = [{'C': [0.01, 0.1, 1, 2], 'penalty': ['l1', 'l2']}]

grid_search = GridSearchCV(LogisticRegression(), param_grid=params, cv = 5)
grid_search.fit(vectors, target)
print("Best params: {}".format(grid_search.best_params_))
print("Best cross validation score: {}".format(grid_search.best_score_))

from sklearn.model_selection import GridSearchCV

params = [{'n_estimators': [1000,  500], 'max_depth': [25, 30]}]

grid_search = GridSearchCV(RandomForestClassifier(), param_grid=params, cv = 5)
grid_search.fit(vectors, target)
print("Best params: {}".format(grid_search.best_params_))
print("Best cross validation score: {}".format(grid_search.best_score_))

rf_vectors = RandomForestClassifier(n_estimators=1000, max_depth=30)
rf_vectors.fit(vectors, target)

xg_vectors = RandomForestClassifier(n_estimators=1000, max_depth=30)
xg_vectors.fit(vectors, target)

from sklearn.preprocessing import OneHotEncoder
vector_predict = rf_vectors.predict(vectors)
vector_predict = vector_predict.reshape(-1, 1)
print("vector predict shape: {}".format(vector_predict.shape))

df['vector_predict'] = vector_predict
df['vector_predict'] = df['vector_predict'].astype(str)
df = pd.get_dummies(df, columns = ['vector_predict'])

"""### Seperate features and Target"""

y = target
df.drop([ 'Application/Permit Number', 'Address', 'Description',  'Contractor' ], axis = 1, inplace = True)

print("Shape of input features: ", df.shape)
df.head()

"""#### Approach two: stacking text vectors, numerical and categorical features"""

X = scipy.sparse.hstack((vectors, df))
print(type(X))
X.shape

"""#### split train data into train and validate"""

X_train, X_validate, y_train, y_validate = train_test_split(df, y, test_size = 0.1, random_state = 10, stratify = y)

"""## Modelling"""

X_train.shape

"""#### Using Random Forest classifier"""

from sklearn.model_selection import GridSearchCV

params = [{'n_estimators': [ 500, 600], 'max_depth': [ 15, 20]}]

grid_search = GridSearchCV(RandomForestClassifier(), param_grid=params, cv = 5)
grid_search.fit(df, target)
print("Best params: {}".format(grid_search.best_params_))
print("Best cross validation score: {}".format(grid_search.best_score_))

rf = RandomForestClassifier(n_estimators=500, max_depth=20)
rf.fit(X_train, y_train)

rf.score(X_validate, y_validate)

xg = XGBClassifier(n_estimators=500, max_depth=20)
xg.fit(X_train, y_train)

xg.score(X_validate, y_validate)

"""#### plotting feature importances"""

features = X_train.columns
importances = rf.feature_importances_
indices = np.argsort(importances)
indices = indices[::-1][:15]
fig = plt.figure(figsize=(12, 7))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.savefig('./feature_imp')

"""### Evaluation Metrics
#### Since it is an imbalanced Data Set, we can evaluate performance by computing precision, recall and F1-score
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.metrics import roc_auc_score,roc_curve,scorer
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score,recall_score, precision_recall_curve
# %matplotlib inline

predictions = rf.predict(X_validate)
conf_matrix = confusion_matrix(y_validate,predictions)
print(conf_matrix)

print(classification_report(y_validate, predictions, target_names = list(le_name_mapping.keys())))

"""#### Using predictions from Xgboost"""

predictions = rf.predict(X_validate)
conf_matrix = confusion_matrix(y_validate,predictions)

print(conf_matrix)

list(le_name_mapping.keys())

print(classification_report(y_validate, predictions, target_names = list(le_name_mapping.keys())))

"""## Predict for Test Data"""

#splitting test data from cleaned train_test dataframe
test_df = train_test[df.shape[0]:]

test_application_number = test_df['Application/Permit Number']
test_df = test_df.drop(['Application/Permit Number'], axis = 1)

test_df['Latitude'].fillna((test_df['Latitude'].mean()), inplace=True)
test_df['Longitude'].fillna((test_df['Longitude'].mean()), inplace=True)

"""#### Encode the test Address data into vector using previously trained Address vectorizer"""

test_df['Address'] = test_df['Address'].astype(str)
test_df['Address'] = test_df['Address'].apply(modify_address)

test_addr_vector = addr_vectorizer.transform(test_df['Address'].tolist())  
test_addr_vector.shape

"""#### encode test Description data using previously trained Description vectorizer"""

test_desc_vector = desc_vectorizer.transform(test_df['Description'].tolist())  
test_desc_vector.shape

"""#### encode test Contractor data using previously trained Contractor vectorizer"""

test_contract_vector = contract_vectorizer.transform(test_df['Contractor'].tolist())  
test_contract_vector.shape

test_vectors = scipy.sparse.hstack((test_addr_vector, test_desc_vector, test_contract_vector))
test_vectors

test_vector_predict = rf_vectors.predict(test_vectors)
test_vector_predict = test_vector_predict.reshape(-1, 1)
print("vector predict shape: {}".format(test_vector_predict.shape))

test_df['vector_predict'] = test_vector_predict
test_df['vector_predict'] = test_df['vector_predict'].astype(str)
test_df = pd.get_dummies(test_df, columns = ['vector_predict'])

test_df.drop(['Contractor', 'Address', 'Description'], axis = 1, inplace=True)

test_df['total period'] = (test_df['Expiration Date'] - test_df['Application Date']).dt.days
test_df['total period'].fillna(diff.mean(), inplace = True) 
test_df = test_df.drop(['Expiration Date','Application Date'] , axis = 1)

print(test_df.shape)
test_df.head()

"""### predict Categories for Test Data

#### We do below step to avoid train and test feature order mismatch
"""

test_df  = test_df[df.columns]

"""#### predict output categories (These are encoded values)"""

test_category = rf.predict(test_df)

"""#### Decode the above output to get Categories"""

test_category = le.inverse_transform(test_category)

"""#### create submission Dataframe and save"""

submission = pd.DataFrame({'Application/Permit Number': test_application_number, 'Category': test_category})

submission.shape

submission.head()

submission.to_csv('./submission_modified.csv', sep = ',', index = False)

files.download('submission_modified.csv')

files.download('category.png')

files.download('./feature_imp.png')

