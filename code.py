# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WTjW-YZR7aDXezmR3xpwHWfrH0pw3Ypk
"""

!pip install xgboost seaborn --quiet

import numpy as np
import pandas as pd
import scipy
import io
#from google.colab import files
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, RepeatedKFold
import seaborn as sns

pd.options.mode.chained_assignment = None

"""#### Read train data"""

#can also import data using these URLs by replacing file path with URL in pd.read_csv
# df_url = "https://raw.githubusercontent.com/RishabhRJain/Building-permit-category-prediction-challenge/master/train_file.csv"
# testdf_url = "https://raw.githubusercontent.com/RishabhRJain/Building-permit-category-prediction-challenge/master/test_file.csv"
df = pd.read_csv('./train.csv', sep = ',')

df.head()

print("Info about Data:")
print("Number of rows: ", df.shape[0])
print("Number of Columns: ", df.shape[1])
print("\nNumber of null values in each column:\n", df.isnull().sum())

y_train = df['Category']
df = df.drop('Category' , axis = 1)

"""#### Read Test Data"""

test_df = pd.read_csv('./test.csv', sep = ',')

print("Info about Data:")
print("Number of rows: ", test_df.shape[0])
print("Number of Columns: ", test_df.shape[1])

"""#### Merge train and test data set"""

train_test = pd.concat([df, test_df], join = 'inner')
train_test.head()

print("Info about Data:")
print("Number of rows: ", train_test.shape[0])
print("Number of Columns: ", train_test.shape[1])
print("\n Number of null values in both train and test data: \n", train_test.isnull().sum())

"""## Data clean

#### Drop columns which we are sparse (Final Date), redundant (Location can be got through Latitude and Longitude features) or if you think they dont help in predicting Target category (Application Id)
"""

del_columns = ['Master Use Permit', 'Permit and Complaint Status URL', 
                'Location', 'Issue Date', 'Final Date']
train_test.drop(del_columns, axis = 1, inplace = True)

"""#### check number of unique values and categories in categorical columns"""

train_test.nunique()

"""#### we see that even Applicant's name is repeated since there are only 14k unique values from sample size of 55k. This means it is generally the contractors or the builders who will file an application. Hence Applicant names are repeated"""

binary_cols =  train_test.nunique()[train_test.nunique().values == 2].keys().tolist()

# Maximum categories are for Action type. Hence threshold for categorical columns has been kept at 20
categorical_cols = train_test.nunique()[train_test.nunique().values <= 20].keys().tolist()
categorical_cols = [col for col in categorical_cols if col not in binary_cols]

date_cols = ['Application Date',  'Expiration Date']

location_cols = ['Latitude', 'Longitude' ]

text_cols = ['Address', 'Description', 'Contractor' ]

print("Binary columns are: ", binary_cols)
print("Categorical columns are: ", categorical_cols)
print("Date columns are: ", date_cols)
print("Location columns are: ", location_cols)
print("Text columns are: ", text_cols)

"""#### convert date_cols to Datetime"""

for col in date_cols:
  train_test[col] = pd.to_datetime(train_test[col])

"""#### Remove or replace NaN values"""

for col in categorical_cols + text_cols:
    train_test[col].fillna('NA', inplace = True)
   
for col in location_cols:
    train_test[col].fillna(train_test[col].mean(), inplace = True)

train_test['Applicant Name'].fillna('', inplace = True)

train_test.isnull().sum()

for col in categorical_cols:
    print("\n{}: {} \n {}".format(col, train_test[col].nunique(), train_test[col].unique()))

"""#### Before one hot encoding, create a train copy to be used for visualisation"""

df_copy = train_test[:len(y_train)]
df_copy['Category'] = y_train

"""#### one hot encode categorical and binary columns in train and test data"""

train_test = pd.get_dummies(train_test, columns = categorical_cols)

le_bin = LabelEncoder()
for col in binary_cols:
    train_test[col] = le_bin.fit_transform(train_test[col])

"""## Visualisation"""

df_copy.head()

catplot = sns.catplot(x="Category", kind="count", data=df_copy)
catplot.set_xticklabels(rotation=45)
catplot.set_titles('Category' )
catplot.savefig('./category.png')

"""##### As we see above, we have an imbalanced Dataset"""

work_plot = sns.catplot(x="Work Type", kind="count", data=df_copy)
work_plot.set_xticklabels(rotation=45)
work_plot.set_titles('Work Type' )

status_plot = sns.catplot(x="Status", kind="count", data=df_copy)
status_plot.set_xticklabels(rotation=90)
status_plot.set_titles('Status' )

print("Top applicants for building permit: \n{}".format(df_copy['Applicant Name'].value_counts()[:5]))

values = df_copy['Applicant Name'].value_counts()[:5].values.tolist()
names = df_copy['Applicant Name'].value_counts()[:5].keys().tolist()
print("names: ", names)
print("values: ", values)
#plt.bar(keys, values)

categories = df_copy['Category'].unique().tolist()
dictionary = {cat:[] for cat in categories}

for name in names:
    cats = df_copy[df_copy['Applicant Name'] == name]['Category'].value_counts().keys().tolist()
    cat_count = df_copy[df_copy['Applicant Name'] == name]['Category'].value_counts().values.tolist()
    
    not_found_categories = [category for category in categories if category not in cats]
    for cat in not_found_categories:
        dictionary[cat].append(0)
        
    for i,cat in enumerate(cats):
            dictionary[cat].append(cat_count[i])
        
print("Top 5 applicant names according to categories: \n", dictionary)

SINGLE_FAMILY = dictionary['SINGLE FAMILY / DUPLEX']
COMMERCIAL = dictionary['COMMERCIAL']
INSTITUTIONAL = dictionary['INSTITUTIONAL']
MULTIFAMILY = dictionary['MULTIFAMILY']
INDUSTRIAL = dictionary['INDUSTRIAL']

ind = np.arange(5)    # the x locations for the groups
width = 0.35       # the width of the bars: can also be len(x) sequence

p1 = plt.bar(ind, SINGLE_FAMILY, width )

p2 = plt.bar(ind, COMMERCIAL, width,
             bottom=SINGLE_FAMILY)

p3 = plt.bar(ind, INSTITUTIONAL, width,
             bottom=COMMERCIAL)

p4 = plt.bar(ind, MULTIFAMILY, width,
             bottom=INSTITUTIONAL)

p5 = plt.bar(ind, INDUSTRIAL, width,
             bottom=MULTIFAMILY)

plt.title('Categories by top applicants')
plt.xlabel('Applicant names')
plt.ylabel('Numer of permits')
plt.xticks(ind, names, rotation = 'vertical')
plt.legend((p1[0], p2[0], p3[0], p4[0], p5[0]), ('SINGLE_FAMILY', 'COMMERCIAL', 'INSTITUTIONAL', 'MULTIFAMILY', 'INDUSTRIAL'))

plt.show()

sns.lmplot( x="Latitude", y="Longitude", data=df_copy, fit_reg=False, hue='Category', legend=True)

"""## Copy clean train data back from train_test dataframe"""

df = train_test[:len(y_train)]
df['Category'] = y_train

"""#### create a date dataframe to understand the time difference between expiration and application date based on Category"""

#drop rows in which Application or Expiration Date is empty
date_df = df.dropna(subset=date_cols, how='any')

date_df['total period'] = (date_df['Expiration Date'] - date_df['Application Date']).dt.days

"""#### We see that different categories have time periods for Expiration Date with Multi family Categories having longest duration"""

diff = date_df.groupby(['Category'])['total period'].mean()
diff

"""#### We will use the above data for imputing NaN date values in train data based on categories"""

# Mean time  for Total period across all Categories. We use this to append NaN values in test data
diff.mean()

"""#### Finding time difference in days for train data

#### Fill NaN values in total period column with mean value based on Category
"""

df['total period'] = (df['Expiration Date'] - df['Application Date']).dt.days

df['total period'] = df.apply(
    lambda row: diff[row['Category']] if np.isnan(row['total period']) else df['total period'], axis=1)

df.drop(date_cols, axis = 1, inplace = True)

"""#### use label encoding for Target variable"""

le = LabelEncoder()
target = le.fit_transform(df['Category'])
df.drop('Category', axis = 1, inplace = True)

le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)

df['Latitude'].fillna((df['Latitude'].mean()), inplace=True)
df['Longitude'].fillna((df['Longitude'].mean()), inplace=True)

df.head()

"""### Encoding Address column into a vector"""

#To convert some float values in Address column to string
df['Address'] = df['Address'].astype(str)

def modify_address(row):
  '''
  Takes complete address and returns only street name without spaces. 
  Modified address can be encoded and used to predict Category
  '''
  addr = row.split(" ", 1)
  if len(addr) > 1:
    addr = addr[1].replace(' ', '')
  else:
    addr = addr[0].replace(' ', '')
  return addr

##For example try this:
#(df['Address'][0].split(" ", 1)[1]).replace(' ', '')

df['Address'] = df['Address'].apply(modify_address)

addr_vectorizer = CountVectorizer(min_df = 2) 
addr_vector = addr_vectorizer.fit_transform(df['Address'].tolist())  
addr_vector

"""### Encoding Description column into a vector

#### Using Count Vectorizer for Description column
"""

desc_vectorizer = CountVectorizer(min_df=5 , stop_words='english') 
desc_vector = desc_vectorizer.fit_transform(df['Description'].tolist())  
desc_vector

"""### Encoding Contractor into a vector

#### Using Count Vectorizer for Contractor column
"""

contract_vectorizer = CountVectorizer(min_df = 2) 
contract_vector = contract_vectorizer.fit_transform(df['Contractor'].tolist())  
contract_vector

"""#### vectorizer for applicant name"""

name_vectorizer = CountVectorizer(min_df=2)
name_vector = name_vectorizer.fit_transform(df['Applicant Name'].tolist())
name_vector

vectors = scipy.sparse.hstack((addr_vector, desc_vector, contract_vector, name_vector))
vectors

"""### Seperate features and Target"""

y = target
df.drop([ 'Application/Permit Number', 'Address', 'Description',  'Contractor', 'Applicant Name' ], axis = 1, inplace = True)

print("Shape of input features: ", df.shape)
df.head()

"""#### Model building approach: stacking text vectors, numerical and categorical features"""

X = scipy.sparse.hstack((vectors, df))
print(type(X))
X.shape

"""#### split train data into train and validate"""

X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size = 0.1, random_state = 10, stratify = y)

X_train.shape

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

params = [{'C': [0.01, 0.1, 1, 2], 'penalty': ['l1', 'l2']}]

grid_search = GridSearchCV(LogisticRegression(), param_grid=params, cv = 5)
grid_search.fit(X, y)
print("Best params: {}".format(grid_search.best_params_))
print("Best cross validation score: {}".format(grid_search.best_score_))

"""#### Using Random Forest classifier"""

from sklearn.model_selection import GridSearchCV

params = [{'n_estimators': [ 500, 600], 'max_depth': [ 15, 20]}]

grid_search = GridSearchCV(RandomForestClassifier(), param_grid=params, cv = 5)
grid_search.fit(df, target)
print("Best params: {}".format(grid_search.best_params_))
print("Best cross validation score: {}".format(grid_search.best_score_))

def CreateBalancedSampleWeights(y_train, largest_class_weight_coef):
    classes = np.unique(y_train, axis = 0)
    classes.sort()
    class_samples = np.bincount(y_train)
    total_samples = class_samples.sum()
    n_classes = len(class_samples)
    weights = total_samples / (n_classes * class_samples * 1.0)
    class_weight_dict = {key : value for (key, value) in zip(classes, weights)}
    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * largest_class_weight_coef
    sample_weights = [class_weight_dict[y] for y in y_train]

    return sample_weights

np.unique(y, axis = 0)

"""#### Since its an imbalanced dataset, we will assign weights to classes based on their occurence. Least occuring class gets more weight and vice versa. This lets the model focus on less frequentlu occuring classes."""

largest_class_weight_coef = max(df_copy['Category'].value_counts().values)/df.shape[0]
weight = CreateBalancedSampleWeights(y_train, largest_class_weight_coef)
weight[:5]

xg = XGBClassifier(n_estimators=1000, weights = weight, max_depth=20)
xg.fit(X_train, y_train)

xg.score(X_validate, y_validate)

"""### Evaluation Metrics
#### Since it is an imbalanced Data Set, we can evaluate performance by computing precision, recall and F1-score
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.metrics import roc_auc_score,roc_curve,scorer
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score,recall_score, precision_recall_curve
# %matplotlib inline

predictions = xg.predict(X_validate)
conf_matrix = confusion_matrix(y_validate,predictions)
print(conf_matrix)

print(classification_report(y_validate, predictions, target_names = list(le_name_mapping.keys())))

"""## Predict for Test Data"""

#splitting test data from cleaned train_test dataframe
test_df = train_test[df.shape[0]:]

test_df['Latitude'].fillna((test_df['Latitude'].mean()), inplace=True)
test_df['Longitude'].fillna((test_df['Longitude'].mean()), inplace=True)

"""#### Encode the test Address data into vector using previously trained Address vectorizer"""

test_df['Address'] = test_df['Address'].astype(str)
test_df['Address'] = test_df['Address'].apply(modify_address)

test_addr_vector = addr_vectorizer.transform(test_df['Address'].tolist())  
test_addr_vector.shape

"""#### encode test Description data using previously trained Description vectorizer"""

test_desc_vector = desc_vectorizer.transform(test_df['Description'].tolist())  
test_desc_vector.shape

"""#### encode test Contractor data using previously trained Contractor vectorizer"""

test_contract_vector = contract_vectorizer.transform(test_df['Contractor'].tolist())  
test_contract_vector.shape

"""#### Encode Applicant name data"""

test_name_vector = name_vectorizer.transform(test_df['Applicant Name'].tolist())
test_name_vector.shape

"""#### Stacking all vectors. Remember to keep the vectors in the same order used while creating training data"""

test_vectors = scipy.sparse.hstack((test_addr_vector, test_desc_vector, test_contract_vector, test_name_vector))
test_vectors

test_application_number = test_df['Application/Permit Number']
test_df.drop([ 'Application/Permit Number', 'Address', 'Description',  'Contractor', 'Applicant Name' ], axis = 1, inplace = True)

test_df['total period'] = (test_df['Expiration Date'] - test_df['Application Date']).dt.days
test_df['total period'].fillna(diff.mean(), inplace = True) 
test_df = test_df.drop(['Expiration Date','Application Date'] , axis = 1)

print(test_df.shape)
test_df.head()

X_test = scipy.sparse.hstack((test_vectors, test_df))
X_test.shape

"""### predict Categories for Test Data

#### We do below step to avoid train and test feature order mismatch
"""

test_df  = test_df[df.columns]

"""#### predict output categories (These are encoded values)"""

test_category = xg.predict(X_test)

"""#### Decode the above output to get Categories"""

test_category = le.inverse_transform(test_category)

"""#### create submission Dataframe and save"""

submission = pd.DataFrame({'Application/Permit Number': test_application_number, 'Category': test_category})

submission.shape

submission.head()

submission.to_csv('./submission_modified.csv', sep = ',', index = False)

